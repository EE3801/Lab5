# Part 4. Maximizing vCPUs

## 17.
In this new pipeline, the number of jobs have greatly increased, so we will want to optimize the pcluster configuration to maximize the number of jobs you can run. Currently, we are using compute nodes that have eight vCPUs. Along with the two vCPUs we have in the master node and the EC2 instance, we can typically only have seven compute nodes before we exceed the 64 vCPUs limit on AWS. That means that we are only using 60 out of the possible 64 vCPUs. 

In order to make use of the remaining four vCPUs, we can add a compute node with four vCPUs to the same queue. We can do this by modifying the following lines in the cluster config file on your EC2 instance to add a `.xlarge` instance instead of `.2xlarge`, e.g. if you are using `r5.2xlarge`:

```yml
Scheduling:
  Scheduler: slurm
  SlurmQueues:
  - Name: queue1
    ComputeResources:
    - Name: r5-2xlarge
      InstanceType: r5.2xlarge
      MinCount: 0
      MaxCount: 10
    - Name: r5-xlarge
      InstanceType: r5.xlarge
      MinCount: 0
      MaxCount: 1
```

If you are using `r5a.2xlarge`, you should add `r5a.xlarge`. Limiting the `.xlarge ` instances to one node makes sure that the bulk of the queue will be made up of `.2xlarge ` compute nodes. 

If you have been using `4xlarge` (16 vCPUs) compute instances, you should add one `2xlarge`(8 vCPUs) and one `xlarge` (4 vCPUs) since the `4xlarge` instances will take a maximum of `16x3=48` vCPUs. That will leave you with 12 vCPUs. So adding one `2xlarge` (8 vCPUs) and one `xlarge` (4 vCPUs) will allow you to make use of all 60 vCPUs available.

## 18.
You can stop your current compute nodes, and update your cluster by doing: 

```shell
[ec2-user@ip-54.169.221.196 ~]
$ pcluster update-compute-fleet --status STOP_REQUESTED --region ap-southeast-1 --cluster-name MyCluster01

[ec2-user@ip-54.169.221.196 ~]
$ pcluster update-cluster --cluster-configuration ~/cluster-config.yaml --cluster-name MyCluster01
```

If you get something like:
```shell
{
  "message": "Update failure",
  "updateValidationErrors": [
    {
      "parameter": "Scheduling.SlurmQueues[queue1].ComputeResources",
      "requestedValue": {
        "Name": "r5a-xlarge",
        "InstanceType": "r5a.xlarge",
        "MinCount": 0,
        "MaxCount": 1
      },
      "message": "All compute nodes must be stopped. Stop the compute fleet with the pcluster update-compute-fleet command"
    }
```

You might have to enter the following command to check on the status of your command to stop the compute nodes:

```shell
[ec2-user@ip-54.169.221.196 ~]
$ pcluster describe-cluster --region ap-southeast-1 --cluster-name MyCluster01
```

Look for the following message:
```shell
  "computeFleetStatus": "STOPPED",
```

At that point, you should be able to redo the `update-cluster` command again.

If you encounter errors submitting jobs after updating the compute fleet, you can try running the following command:

```shell
[ec2-user@ip-54.169.221.196 ~]
$ pcluster update-compute-fleet --status START_REQUESTED --region ap-southeast-1 --cluster-name MyCluster01
```

## 19.
You are now ready to submit the jobs. Remember to remove the files created during the last run before running `pipe2a.sh` from the `20181105` directory.

## 20.
Check that the jobs were submitted successfully, and then submit the `consol_jobs.sh` script with a dependence on the six jobs submitted by the `pipe2a.sh` script (replace the job numbers with those in your queue). In your `consol_job.sh` script, you may want to change `--dependency=afterok` to `--dependency=afterany`, which will run the command when all the dependency jobs have completed regardless of whether they succeeded or failed.

```shell
(env1) [ec2-user@ip-10-0-5-43 20181105]
$ sbatch --dependency=afterok:11:12:13:14:15:16 /data/src/PyHipp/consol_jobs.sh
```

Remember to update the `InstanceID` in your Lambda function with the `InstanceID` of your head node so the head node can be terminated once the jobs are done.

## 21.
You should receive a notification when the first compute node starts up, which will run the `rplparallel` and `rse` jobs. When the `rse` job finishes, you should get another notification, followed by notifications when four additional compute nodes get started to run the `rs?a` jobs. Once several `rpllfp` and `rplhighpass-sort` jobs get submitted, you will receive notifications that three additional compute nodes have started up for a total of 8 compute nodes totaling 60 vCPUs.


## 22.
You can use the following command to keep track of what is happening with the various jobs:

```shell
(env1) [ec2-user@ip-10-0-5-43 20181105]
$ tail -f rs?a*slurm*out
```

or (replace rplhps below with the prefix for your slurm output and error files):

```shell
(env1) [ec2-user@ip-10-0-5-43 20181105]
$ find session01 -name "rplhps*slurm*out" | xargs tail -f 
```

Once all eight compute notes have started up and running jobs, you can check your queue with the following command to see that you have instances with both 4 vCPUs and 8 vCPUs running that add up to 60 vCPUs:

```shell
(env1) [ec2-user@ip-10-0-5-43 20181105]
$ squeue | grep R
```

These two commands will find all the lines containing `R` (indicating jobs that are running under the ST column in the output of squeue). You can use the following command to count the number of jobs running, but since the previous command will include two header rows, you will have to subtract two from the output:

```shell
(env1) [ec2-user@ip-10-0-5-43 20181105]
$ squeue | grep R | wc -l
```

## 23.
If you get `DependencyNeverSatisfied` for some of your jobs, check the error files for your earlier jobs to see if there were any errors (e.g. if you see `You must specify a regionâ€¦` that probably means that you forgot to copy over your AWS credentials). In order to get proper time measurements for this setup, you will have to delete the remaining queued jobs, remove the files created, and re-submit `pipe2a.sh`.

## 24.
You can take a break here and wait till you receive the email notification that the snapshot has been completed to continue.
