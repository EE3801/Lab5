# Part 6. Processing all remaining data

## 31.
You can now process the data from the other days with neural data by doing:

```shell
(env1) [ec2-user@ip-10-0-5-43 20181105]
$ cd ..

(env1) [ec2-user@ip-10-0-5-43 picasso]
$ for i in 2018110[12]; do echo $i; cd $i; bash /data/src/PyHipp/pipe2a.sh; cd ..; done
```

This series of commands will search for directories `20181101` and `20181102`, iterate over each directory by first printing the directory name, changing to that directory, calling the `pipe2a.sh` script, going up one directory, before repeating.

## 32.
For the other days in the `picasso` directory, we only need to run the `rplparallel-slurm.sh` script as they do not contain neural data. However, to avoid receiving notification emails for each of the 42 days, add a `#`  to the start of the `aws sns …` command to comment the command out. You can then submit the jobs by doing:

```shell
(env1) [ec2-user@ip-10-0-5-43 picasso]
$ for i in 20180??? 201810??; do echo $i; cd $i; sbatch /data/src/PyHipp/rplparallel-slurm.sh; cd ..; done
```

## 33.
We will also need to submit the script to take a snapshot once all the jobs are done. In order to do that, we will need to find the jobs that will spawn more jobs (i.e. `rplsplit` jobs), and wait for them to finish before we submit the job that will wait for all the jobs left in the queue to be completed before initiating the snapshot:

```shell
(env1) [ec2-user@ip-10-0-5-43 picasso]
$ squeue | grep -e rs1a -e rs2a -e rs3a -e rs4a


920   compute      rs1a ec2-user PD       0:00      1 (Resources) 
921   compute      rs2a ec2-user PD       0:00      1 (Resources) 
922   compute      rs3a ec2-user PD       0:00      1 (Resources) 
923   compute      rs4a ec2-user PD       0:00      1 (Resources) 
924   compute      rs1a ec2-user PD       0:00      1 (Resources) 
925   compute      rs2a ec2-user PD       0:00      1 (Resources) 
926   compute      rs3a ec2-user PD       0:00      1 (Resources) 
927   compute      rs4a ec2-user PD       0:00      1 (Resources)

(env1) [ec2-user@ip-10-0-5-43 picasso]
$ sbatch --dependency=afterok:4:5:6:7:10:11:12:13 /data/src/PyHipp/consol_jobs.sh
```

We do not need to add the `rplparallel-slurm.sh` jobs as they will be captured by the `consol_jobs.sh` script when the `consol_jobs.sh` script runs after the `rplsplit` jobs are done.

## 34.
The last thing to do is to replace the instance IDs in your Lambda function with those of your cluster’s head node.