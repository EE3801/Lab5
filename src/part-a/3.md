# Part 3

## 9.
Each instance of `MountainSort` (the spike sorting software we are using) tries to obtain exclusive access numerous times to a `processor_specs.json` file in `/data/miniconda3/envs/cenv3/etc/mountainlab/database` by locking the file, presumably so the file cannot be modified while it is being read.

However, when there are a lot of jobs trying to lock the file, they end up having to wait for each other to release the file. This means that we will need to resolve this issue in order to be able perform spike sorting simultaneously on multiple channels properly. 

One way to get around the file locking issues is to create separate conda environments that each contain a copy of the `processor_specs.json` file so we can run the spike sorting software in parallel. In order to do this, we will need to create multiple clones of the `env1` environment that we have been using. We could create a cloned environment for every spike sorting job (i.e. one for each of the 110 channels), but since the maximum number of vCPUs we can use at one time is 64, it will be wasteful to create more than 64 clones. 

So in order to reuse the environments, we will need a way to make sure that each job is using a different environment, and jobs that have finished running can return the environment they used so other jobs can use them. This can be done by creating a file containing a list of available environments that jobs can access at the beginning to find an available environment, remove that environment from the list so other jobs cannot use the same environment, and add the environment back to the list when the sorting is done. In order to prevent the file from being changed while it is being read, this method will require creating a lock on the file containing the list of environments temporarily so only one job can modify it at a time.

## 10.
Use the nano editor to modify the `envlist.py` file in the `PyHipp` directory to complete the code (look for the lines with `#add code here`) to allow it to create a list of environment names (`cenv0`, `cenv1`, `cenv2`, â€¦ ,`cenv63`) by doing (this command is for illustration only, so do not run the command on your cluster yet):

```shell
(env1) [ec2-user@ip-10-0-5-43 20181105] $ /data/src/PyHipp/envlist.py cenv 64
```

The list will essentially look like:

```shell
cenv0
cenv1
cenv2
...
cenv63
```

This list will be saved into `/data/picasso/envlist.hkl` using the hickle module. 

When you want to use an environment, you should be able to do (again, these commands are meant to explain what we are trying to do, so do not run them yet):

```shell
(env1) [ec2-user@ip-10-0-5-43 20181105] $ /data/src/PyHipp/envlist.py
```

which should return:

```shell
cenv0
```

In this example, the first item on the list will be removed from the list and returned. The next time the function is called again in this form, the environment name `cenv1` will be removed from the list and returned. This makes sure that each environment will only be used by one job at a time.

When the spike sorting is completed, the environment can be added back to the list by doing (as above, do not run this command yet):

```shell
(env1) [ec2-user@ip-10-0-5-43 20181105] $ /data/src/PyHipp/envlist.py cenv0
```

This will append `cenv0` to the end of the list so it can be used by another job.

## 11.
Once you have completed modifying the code, you will need to install the `FileLock` module on your cluster before you can test the code:

```shell
(env1) [ec2-user@ip-10-0-5-43 20181105] $ pip install filelock
```

## 12.
After testing your code by first creating the list of environments, requesting an environment, and then returning that environment (the commands described above in [Step 10](3.md#10)), you can load the `envlist.hkl` file in `iPython` to check the list contained in the file:

```shell
(env1) [ec2-user@ip-10-0-5-43 20181105] $ ipython

In[ ]: import hickle
In[ ]: clist = hickle.load('/data/picasso/envlist.hkl')
In[ ]: len(clist)
In[ ]: clist
```

> <p class="task"> Task
>
> Include your completed envlist.py script, screenshots of the terminal window where you ran the three ways to call your script, and the resulting list in the envlist.hkl file in your lab report.

## 13.
Add `envlist.py` to your repository and push the change to GitHub again.

## 14.
Your current snapshot already contains multiple clones of the `env1` environment. You can check the environments by doing:

```shell
(env1) [ec2-user@ip-10-0-5-43 20181105] $ conda env list
```

You should see a list of 66 environments: 

```shell
# conda environments:
#
base                     /data/miniconda3
cenv0                    /data/miniconda3/envs/cenv0
cenv1                    /data/miniconda3/envs/cenv1
cenv10                   /data/miniconda3/envs/cenv10
...
```

Run the `envlist.py` program to create the files `/data/picasso/envlist.hkl` and `/data/picasso/envlist.hkl.lock`:

```shell
(env1) [ec2-user@ip-10-0-5-43 20181105] $ /data/src/PyHipp/envlist.py cenv 64
```

Just for your information, the clones of the `env1` environment were created using a while-loop in bash that incremented `x=0` to `x=63` (`you do not need to do the following`):

```shell
(env1) [ec2-user@ip-10-0-5-43 20181105] $ x=0; while [ $x -le 63 ]; do echo $x; conda create --name cenv$x --clone env1 --copy; (( x++ )); done; aws sns publish --topic-arn arn:aws:sns:ap-southeast-1:123456789012:awsnotify --message "CondaCreateComplete"
```

We used the `--copy` argument since by default conda creates clones of environments by creating hard links to the files in the original environment, which will again create conflicts when multiple MountainSort jobs try to obtain exclusive access to the same file.

## 15.
Now we will need to modify `rplhighpass-sort-slurm.sh` to make use of the separate conda environments by adding these lines before the python command:

```bash
/data/miniconda3/bin/conda init
source ~/.bashrc
envarg=`/data/src/PyHipp/envlist.py`
conda activate $envarg
```

As well as the lines below after the python command:

```bash
conda deactivate 
/data/src/PyHipp/envlist.py $envarg
```

## 16.
Push your changes again to GitHub.

